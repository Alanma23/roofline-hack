# Next Steps: Theory ‚Üí Kernel ‚Üí Validation

**Your path from understanding roofline to proving it works.**

---

## üéØ What You'll Do

1. **Understand theory** - Run roofline calculator (predictions)
2. **Write a kernel** - Implement simple GEMV benchmark
3. **Measure reality** - Benchmark on your Jetson
4. **Compare** - Does theory match reality?

---

## üìù Step 1: Understand Roofline Theory (Local, 5 min)

Run the theoretical calculator to see predictions:

```bash
cd ~/roofline-hack
python3 src/roofline/calculator.py
```

**Expected output:**
```
======================================================================
Roofline Predictions: 4096√ó4096 GEMV on Jetson Orin Nano
Peak BW: 60.0 GB/s
======================================================================

[FP16]
  AI: 1.00 FLOP/byte (critical: 45.00)
  Predicted time: 280.0 Œºs
  Predicted throughput: 0.060 TFLOPS
  Bottleneck: memory
  
[INT8]
  AI: 2.00 FLOP/byte (critical: 91.67)
  Predicted time: 145.0 Œºs
  Predicted throughput: 0.116 TFLOPS
  Bottleneck: memory

Predicted Speedups vs FP16:
  FP16: 1.00√ó
  INT8: 1.93√ó  ‚Üê Theory says 2√ó faster
  INT4: 3.57√ó  ‚Üê Theory says 3.6√ó faster
```

**What this tells you:**
- All precisions are memory-bound (AI << Critical AI)
- Lower precision = less bandwidth = proportional speedup
- INT8 should be ~2√ó faster than FP16

**Now test if theory is right!**

---

## üîß Step 2: Write & Run Simple Kernel (Jetson, 10 min)

Transfer code to Jetson and benchmark:

```bash
# On Jetson
cd ~/roofline-hack

# Install dependencies
pip3 install torch pandas --user

# Run simple kernel benchmark
python3 benchmarks/simple_kernel.py
```

**Expected output:**
```
======================================================================
Benchmarking 4096√ó4096 GEMV on Orin
======================================================================

Running FP16...
  Time: 265.0 Œºs
  TFLOPS: 0.064
  Bandwidth: 51.2 GB/s
  AI: 1.00 FLOP/byte

Running INT8...
  Time: 139.0 Œºs
  TFLOPS: 0.121
  Bandwidth: 49.8 GB/s
  AI: 2.00 FLOP/byte

INT8 Speedup: 1.91√ó (theoretical: 2.0√ó)  ‚Üê Very close!
```

**What you learned:**
- You wrote a kernel (SimpleGEMV.run_kernel)
- You measured real performance
- INT8 is actually ~1.9√ó faster (close to theory!)

---

## üìä Step 3: Compare Theory vs Reality (Jetson, 5 min)

Run the comparison script:

```bash
python3 compare_theory_vs_reality.py
```

**Expected output:**
```
THEORY VS REALITY: Roofline Validation
================================================================================
Hardware: Orin
Problem Size: 4096√ó4096 GEMV

Precision    Predicted (Œºs)   Measured (Œºs)    Error      Bottleneck  
--------------------------------------------------------------------------------
FP16         280.0            265.0            5.4%       memory      
INT8         145.0            139.0            4.1%       memory      

================================================================================
ANALYSIS
================================================================================

1. Speedup (FP16 ‚Üí INT8):
   Predicted: 1.93√ó
   Measured:  1.91√ó
   Error:     1.0%

2. Prediction Accuracy:
   Mean error: 4.7%
   ‚úÖ EXCELLENT (<10% error)

3. Memory Bound Confirmation:
   FP16: AI=1.00, Critical=45.00 ‚Üí 45√ó below (memory-bound ‚úì)
   INT8: AI=2.00, Critical=91.67 ‚Üí 46√ó below (memory-bound ‚úì)

4. Key Insight:
   INT8 is 1.91√ó faster (vs predicted 1.93√ó)
   This proves: Lower precision ‚Üí proportional speedup (memory-bound regime)
```

**What this validates:**
- ‚úÖ Roofline model is accurate (<5% error)
- ‚úÖ Memory bandwidth is the bottleneck
- ‚úÖ Lower precision delivers proportional speedup

---

## üöÄ Step 4: Extend to INT4 (Your Next Challenge)

Now that INT8 works, try INT4!

**Modify the kernel:**

```python
# In benchmarks/simple_kernel.py
# Add INT4 support (need to pack 2 values per byte)

class SimpleGEMV:
    def __init__(self, N, K, precision, device='cuda'):
        # ... existing code ...
        
        if precision == 'int4':
            # Pack two INT4 values per INT8
            # This is your challenge: implement INT4 packing!
            pass
```

**Predicted result:**
```
INT4 Speedup: ~3.6√ó vs FP16
```

**Your task:**
1. Implement INT4 packing in the kernel
2. Benchmark it
3. Check if you get 3.6√ó speedup
4. **Critical:** Measure accuracy impact (run a small model)

---

## üìñ Understanding Your Code

### 1. Roofline Calculator (`src/roofline/calculator.py`)

**Key function:** `predict_gemv(N, K, precision)`

```python
# Calculates:
ai = flops / bytes  # Arithmetic intensity
time_memory = bytes / bandwidth  # Memory-bound time
time_compute = flops / peak_flops  # Compute-bound time
predicted_time = max(time_memory, time_compute)  # Bottleneck
```

**This is the theory:** Performance limited by slower of memory or compute.

### 2. Simple Kernel (`benchmarks/simple_kernel.py`)

**Key function:** `run_kernel()`

```python
def run_kernel(self):
    if self.precision == 'fp16':
        return torch.matmul(self.W, self.x)  # Native FP16
    else:  # int8
        W_fp16 = self.W.to(torch.float16)  # Cast to FP16
        x_fp16 = self.x.to(torch.float16)
        return torch.matmul(W_fp16, x_fp16)  # Compute in FP16
```

**This is reality:** PyTorch calls CUDA kernels, you measure actual time.

### 3. Comparison (`compare_theory_vs_reality.py`)

**What it does:**
1. Runs roofline calculator (theory)
2. Runs benchmark kernel (reality)
3. Calculates error: `(measured - predicted) / predicted √ó 100%`

**If error < 10%:** Model is accurate!

---

## üéì What You're Learning

### Theoretical Understanding:
- ‚úÖ Arithmetic Intensity formula: `AI = FLOPs / Bytes`
- ‚úÖ Roofline model: `time = max(bytes/BW, flops/peak)`
- ‚úÖ Memory-bound regime: AI << Critical AI ‚Üí BW limited
- ‚úÖ Precision scaling: `AI ‚âà 16 / weight_bits`

### Practical Skills:
- ‚úÖ Implement a GPU kernel (GEMV)
- ‚úÖ Benchmark with CUDA events
- ‚úÖ Calculate FLOPS and bandwidth
- ‚úÖ Validate theoretical models

### Research Skills:
- ‚úÖ Hypothesis: Lower bits ‚Üí proportional speedup
- ‚úÖ Experiment: Measure FP16 vs INT8
- ‚úÖ Analysis: Compare predicted vs measured (<5% error)
- ‚úÖ Conclusion: Theory validated ‚úì

---

## üî¨ Advanced Experiments

Once basics work, try:

### 1. Different Problem Sizes
```bash
python3 compare_theory_vs_reality.py --N 2048 --K 2048
python3 compare_theory_vs_reality.py --N 8192 --K 8192
```

**Question:** Does prediction accuracy change with size?

### 2. Power Modes
```bash
# Max performance (15W)
sudo nvpmodel -m 0
python3 compare_theory_vs_reality.py

# Power efficient (7W)
sudo nvpmodel -m 1
python3 compare_theory_vs_reality.py
```

**Question:** Does roofline still predict correctly at lower power?

### 3. Prefill vs Decode
```bash
# Decode: T=1 (current)
# Prefill: T=S (batch processing)

# Modify calculator.py to add predict_gemm() for prefill
# Compare: Is prefill compute-bound or memory-bound?
```

**Hypothesis:** Prefill has higher AI ‚Üí might be compute-bound.

---

## üéØ Success Criteria

**You've succeeded when:**
- ‚úÖ Roofline predictions within 10% of measurements
- ‚úÖ INT8 delivers ~2√ó speedup (validated)
- ‚úÖ You understand why (memory bandwidth bottleneck)
- ‚úÖ You can explain: "AI < Critical AI ‚Üí memory-bound"

**Bonus:**
- ‚úÖ INT4 implemented and validated
- ‚úÖ Accuracy analysis (perplexity test)
- ‚úÖ Power-aware analysis (7W vs 15W)

---

## üìö Files Created

```
roofline-hack/
‚îú‚îÄ‚îÄ src/roofline/
‚îÇ   ‚îî‚îÄ‚îÄ calculator.py          # Theory: Roofline predictions
‚îú‚îÄ‚îÄ benchmarks/
‚îÇ   ‚îî‚îÄ‚îÄ simple_kernel.py       # Practice: Simple GEMV kernel
‚îú‚îÄ‚îÄ compare_theory_vs_reality.py   # Validation: Compare both
‚îî‚îÄ‚îÄ NEXT_STEPS.md              # This guide
```

**Start here:** Run each file in order (theory ‚Üí kernel ‚Üí compare)

---

## ‚ùì Troubleshooting

**"CUDA not available"**
‚Üí Make sure you're running on Jetson with JetPack installed

**"Import error: No module named 'roofline'"**
‚Üí Run from project root: `cd ~/roofline-hack && python3 compare_theory_vs_reality.py`

**"Predictions way off (>20% error)"**
‚Üí Check hardware specs in calculator.py (bandwidth might be different)

---

## üéâ What's Next?

After validating INT8:

1. **INT4 implementation** - Your challenge!
2. **Accuracy testing** - Does INT4 maintain quality?
3. **Full model benchmark** - Run TinyLlama at INT8/INT4
4. **Documentation** - Write up your findings

**You now have the full toolkit: theory ‚Üí implementation ‚Üí validation** üöÄ
