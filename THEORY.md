# Roofline Theory & Precision Formats

Deep dive into the roofline performance model and precision format catalog for GB10.

## Table of Contents

1. [Roofline Model Fundamentals](#roofline-model-fundamentals)
2. [Precision Format Catalog](#precision-format-catalog)
3. [Per-Operator Math](#per-operator-math)
4. [Hardware Support Matrix](#hardware-support-matrix)

---

## Roofline Model Fundamentals

### The Core Formula

```
time = max(time_memory, time_compute)

where:
  time_memory  = Bytes / Bandwidth
  time_compute = FLOPs / Peak_FLOPS
```

**Arithmetic Intensity (AI)** determines which limit applies:

```
AI = FLOPs / Bytes

if AI < Critical_AI:  memory-bound (bandwidth limits)
if AI > Critical_AI:  compute-bound (FLOPS limits)

Critical_AI = Peak_FLOPS / Bandwidth
```

### GB10 Critical AI Values

For NVIDIA GB10 (287 GB/s bandwidth):

| Precision | Peak TFLOPS | Critical AI (FLOP/byte) |
|-----------|-------------|-------------------------|
| FP32 | 31 | 108 |
| FP16 | 62 | 216 |
| FP8 | 124 | 432 |
| INT8 | 124 | 432 |
| INT4 | 248 | 864 |
| NVFP4 | 1000 | 3484 |

**Key insight:** Most transformer operations have AI < 10, making them heavily memory-bound on GB10.

### GEMV (Matrix-Vector, Decode)

```
y[N] = W[N,K] @ x[K]

FLOPs = 2 Ã— N Ã— K
Bytes = K Ã— bpe + N Ã— K Ã— bpe + N Ã— 2
      = (K + NÃ—K + N) Ã— bpe     (assuming same precision)
      â‰ˆ N Ã— K Ã— bpe              (for large N, K)

AI = 2 Ã— N Ã— K / (N Ã— K Ã— bpe) â‰ˆ 2 / bpe

For FP16 (bpe=2):  AI â‰ˆ 1 FLOP/byte  â†’ memory-bound
For FP8 (bpe=1):   AI â‰ˆ 2 FLOP/byte  â†’ still memory-bound
For NVFP4 (bpe=0.5): AI â‰ˆ 4 FLOP/byte â†’ still memory-bound
```

**Speedup from lower precision = bytes reduction ratio** (in memory-bound regime).

### GEMM (Matrix-Matrix, Prefill)

```
C[M,N] = A[M,K] @ B[K,N]

FLOPs = 2 Ã— M Ã— N Ã— K
Bytes = M Ã— K Ã— bpe + K Ã— N Ã— bpe + M Ã— N Ã— bpe

AI = 2 Ã— M Ã— N Ã— K / ((MÃ—K + KÃ—N + MÃ—N) Ã— bpe)
   = 2 Ã— M Ã— N Ã— K / (KÃ—(M+N) + MÃ—N) / bpe

For M = N = K (square):  AI = 2K / (3Ã—bpe)
For K=4096, FP16:       AI = 1365 FLOP/byte â†’ compute-bound!
```

Large batch prefill becomes compute-bound, so lower precision helps less.

---

## Precision Format Catalog

### Why Lower Precision Matters

**The tradeoff:**
- âœ… Lower bits = faster (proportional to bytes reduction)
- âš ï¸ Lower bits = less accurate (quantization error increases)
- ðŸŽ¯ Question: Where's the sweet spot?

### Format Comparison

| Format | Bits | Bytes/elem | Predicted Speedup | Accuracy Loss | GB10 Support |
|--------|------|------------|-------------------|---------------|--------------|
| FP32 | 32 | 4.0 | 1.0Ã— (baseline) | None | âœ“ |
| FP16 | 16 | 2.0 | 2.0Ã— | Minimal | âœ“ |
| BF16 | 16 | 2.0 | 2.0Ã— | Minimal | âœ“ |
| FP8 E4M3 | 8 | 1.0 | 4.0Ã— | <1% | âœ“âœ“ (native TC) |
| FP8 E5M2 | 8 | 1.0 | 4.0Ã— | <1% | âœ“âœ“ (native TC) |
| INT8 | 8 | 1.0 | 4.0Ã— | 1-2% (w/ PTQ) | âœ“ |
| INT4 | 4 | 0.5 | 8.0Ã— | 2-5% | âœ“ |
| NVFP4 | ~4.5 | ~0.56 | ~7.0Ã— | 1-2% | âœ“âœ“ (1000 TFLOPS!) |
| MXFP4 | ~4.25 | ~0.53 | ~7.5Ã— | 2-3% | âœ“ |

**âœ“âœ“** = native tensor core support with exceptional performance

### Scalar Floating Point Formats

| Format | Bits | Layout (E/M) | Range | Bytes/elem |
|--------|------|--------------|-------|------------|
| FP64 | 64 | E11M52 | Â±1.8e308 | 8.000 |
| FP32 | 32 | E8M23 | Â±3.4e38 | 4.000 |
| TF32 | 19* | E8M10 | Â±3.4e38 | 2.375 |
| BF16 | 16 | E8M7 | Â±3.4e38 | 2.000 |
| FP16 | 16 | E5M10 | Â±65504 | 2.000 |
| FP8 E4M3 | 8 | E4M3 | Â±448 | 1.000 |
| FP8 E5M2 | 8 | E5M2 | Â±57344 | 1.000 |

*TF32 uses 32-bit container but only 19 significant bits

### Block Floating Point - OCP MX Formats

Block size = 32 elements, E8M0 scale (power-of-two only)

| Format | Element Bits | Scale Bits | Eff bits/elem | Overhead |
|--------|--------------|------------|---------------|----------|
| MXFP8 E4M3 | 8 | 8 (shared) | 8.250 | 3.0% |
| MXFP8 E5M2 | 8 | 8 (shared) | 8.250 | 3.0% |
| MXFP6 E3M2 | 6 | 8 (shared) | 6.250 | 4.0% |
| MXFP6 E2M3 | 6 | 8 (shared) | 6.250 | 4.0% |
| MXFP4 | 4 | 8 (shared) | 4.250 | 5.9% |
| MXINT8 | 8 | 8 (shared) | 8.250 | 3.0% |

### NVIDIA NVFP4 Format

Block size = 16 elements (2Ã— more scales than MXFP4)

| Component | Bits | Description |
|-----------|------|-------------|
| Element | 4 | E2M1 (8 positive values + negatives + zero) |
| Per-block scale | 8 | E4M3 (non-power-of-two, smoother) |
| Per-tensor scale | 32 | FP32 (huge dynamic range) |
| **Effective bits/elem** | **4.531** | **(11.7% overhead)** |

**Key advantages over MXFP4:**
- Smaller blocks (16 vs 32) = finer-grained adaptation
- E4M3 scales (vs E8M0) = non-power-of-two values
- Two-level scaling = enormous effective range
- Better accuracy (<1% loss vs FP8) at cost of 6.6% more storage

### FP4 E2M1 Codebook

8 representable positive values (sign bit doubles to 16 total):

```
Value: 0.0  0.5  1.0  1.5  2.0  3.0  4.0  6.0
Step:  ---  0.5  0.5  0.5  0.5  1.0  1.0  2.0
```

**Non-uniform spacing:** Fine near zero (0.5), coarse near max (2.0).

The "4/6 algorithm" chooses between max=6 (standard) or max=4 scaling per block to minimize MSE.

### NF4 (bitsandbytes Lookup Table)

Block size = 64 elements, FP16 absmax scale

| Component | Bits | Description |
|-----------|------|-------------|
| Element | 4 | Index into 16-entry lookup table |
| Per-block scale | 16 | FP16 absmax |
| **Effective bits/elem** | **4.250** | **(5.9% overhead)** |

Non-uniform codebook optimized for normally-distributed weights.

### Integer Formats

| Format | Bits | Range | Bytes/elem | Notes |
|--------|------|-------|------------|-------|
| INT8 | 8 | Â±127 | 1.000 | Uniform quantization, per-tensor or per-channel scales |
| INT4 | 4 | Â±7 | 0.500 | Often used with FP16 dequant (W4A16) |
| INT2 | 2 | Â±1 | 0.250 | Research only, severe accuracy loss |

---

## Per-Operator Math

Exact FLOP and byte counts for transformer operations.

### Notation

| Symbol | Meaning |
|--------|---------|
| B | Batch size |
| S | Total sequence length (KV cache size) |
| T | Tokens being processed (= S for prefill, = 1 for decode) |
| H | Hidden dimension (d_model) |
| n_h | Number of query heads |
| n_kv | Number of KV heads (< n_h for GQA) |
| d_h | Head dimension (= H / n_h) |
| d_kv | Total KV dimension (= n_kv Ã— d_h) |
| d_ff | FFN intermediate dimension |
| w_B | Bytes per weight element |
| a_B | Bytes per activation element |
| kv_B | Bytes per KV cache element |

### Attention Linear Projections (Q, K, V, O)

Standard GEMM: C[M,N] = A[M,K] Ã— W[K,N]

| Projection | M | N | K | FLOPs | Bytes |
|------------|---|---|---|-------|-------|
| Q | BÃ—T | H | H | 2Â·BÂ·TÂ·HÂ² | BÂ·TÂ·HÂ·a_B + HÂ²Â·w_B + BÂ·TÂ·HÂ·a_B |
| K | BÃ—T | d_kv | H | 2Â·BÂ·TÂ·HÂ·d_kv | BÂ·TÂ·HÂ·a_B + HÂ·d_kvÂ·w_B + BÂ·TÂ·d_kvÂ·a_B |
| V | BÃ—T | d_kv | H | 2Â·BÂ·TÂ·HÂ·d_kv | Same as K |
| O | BÃ—T | H | H | 2Â·BÂ·TÂ·HÂ² | BÂ·TÂ·HÂ·a_B + HÂ²Â·w_B + BÂ·TÂ·HÂ·a_B |

**Decode AI (T=1):**
```
AI â‰ˆ 2Â·HÂ² / (HÂ²Â·w_B + HÂ·a_B + HÂ·a_B) â‰ˆ 2 / w_B

For FP16 weights: AI â‰ˆ 1 FLOP/byte â†’ memory-bound
```

### Attention Score Computation (QK^T)

Batched matmul: Score[B,n_h,T,S] = Q[B,n_h,T,d_h] Ã— K^T[B,n_kv,d_h,S]

**FLOPs:** 2 Â· B Â· n_h Â· T Â· S Â· d_h

**Bytes:**
- Read Q: B Â· n_h Â· T Â· d_h Â· a_B
- Read K from KV cache: B Â· n_kv Â· S Â· d_h Â· kv_B
- Write scores: B Â· n_h Â· T Â· S Â· a_B

**Prefill AI (T=S):** â‰ˆ 2Â·d_h / (a_B + kv_BÂ·(n_kv/n_h) + a_B)
- For d_h=128, FP16: AI â‰ˆ 42.7 FLOP/byte â†’ **compute-bound**

**Decode AI (T=1):** For large S, KV read dominates
- AI â†’ 2Â·d_h / (d_hÂ·kv_BÂ·(n_kv/n_h) + a_B)
- This is the **KV cache wall** - bytes grow linearly with context length

### Score Ã— V

Batched matmul: Out[B,n_h,T,d_h] = Score[B,n_h,T,S] Ã— V[B,n_kv,S,d_h]

Similar structure to QK^T. V read from KV cache at kv_B precision.

### Softmax

**FLOPs:** ~5 Â· B Â· n_h Â· T Â· S
**Bytes:** 2 Â· B Â· n_h Â· T Â· S Â· a_B
**AI:** â‰ˆ 2.5 â†’ always memory-bound

### FFN Block (SwiGLU / LLaMA-style)

Three projections: gate, up, down

| Projection | M | N | K | FLOPs | Weight Bytes |
|------------|---|---|---|-------|--------------|
| Gate | BÃ—T | d_ff | H | 2Â·BÂ·TÂ·HÂ·d_ff | HÂ·d_ffÂ·w_B |
| Up | BÃ—T | d_ff | H | 2Â·BÂ·TÂ·HÂ·d_ff | HÂ·d_ffÂ·w_B |
| Down | BÃ—T | H | d_ff | 2Â·BÂ·TÂ·HÂ·d_ff | HÂ·d_ffÂ·w_B |

**SiLU + elementwise multiply:**
- FLOPs: 3 Â· B Â· T Â· d_ff
- Bytes: 3 Â· B Â· T Â· d_ff Â· a_B

**Total FFN FLOPs:** 6 Â· B Â· T Â· H Â· d_ff

For LLaMA (d_ff â‰ˆ 8/3 Â· H): FFN â‰ˆ 16 Â· B Â· T Â· HÂ²

**FFN is ~2/3 of total layer compute** in LLaMA models.

### Elementwise Operations

| Op | FLOPs | Bytes | AI |
|----|-------|-------|----|
| RMSNorm | 5Â·BÂ·TÂ·H | 2Â·BÂ·TÂ·HÂ·a_B | ~2.5 |
| Residual add | BÂ·TÂ·H | 3Â·BÂ·TÂ·HÂ·a_B | ~0.33 |
| RoPE | 6Â·BÂ·TÂ·H | 2Â·BÂ·TÂ·HÂ·a_B | ~3 |

All have AI < 5 â†’ always memory-bound. Prime candidates for operator fusion.

### Full Model Totals

**Total FLOPs per forward pass:**
```
Prefill: L Ã— (8Â·BÂ·SÂ·HÂ² + 4Â·BÂ·n_hÂ·SÂ²Â·d_h + 6Â·BÂ·SÂ·HÂ·d_ff) + 2Â·BÂ·SÂ·HÂ·V
Decode:  L Ã— (8Â·BÂ·HÂ² + 4Â·BÂ·n_hÂ·SÂ·d_h + 6Â·BÂ·HÂ·d_ff) + 2Â·BÂ·HÂ·V
```

**Total weight bytes:**
```
L Ã— (4Â·HÂ² + 2Â·HÂ·d_kv + 3Â·HÂ·d_ff) Ã— w_B + HÂ·VÂ·w_B
```

**KV cache bytes per decode step:**
```
L Ã— 2 Ã— n_kv Ã— S Ã— d_h Ã— kv_B
```

### KV Cache Crossover Point

Context length where KV cache reading overtakes weight loading:

```
S_cross = [L Ã— (4HÂ² + 2HÂ·d_kv + 3HÂ·d_ff) Ã— w_B + HÂ·VÂ·w_B] / [L Ã— 2 Ã— n_kv Ã— d_h Ã— kv_B]
```

| Model | FP16 W + FP16 KV | FP8 W + FP8 KV | FP8 W + NVFP4 KV |
|-------|------------------|----------------|-------------------|
| Llama-3 8B | ~1600 tokens | ~4096 tokens | ~7300 tokens |
| Llama-2 70B | ~12800 tokens | ~32000 tokens | ~57000 tokens |

**For long-context generation (10K+ tokens), KV cache quantization becomes critical.**

---

## Hardware Support Matrix

### GB10 Grace Blackwell (Primary Target)

| Format | Peak TFLOPS | Native TC | Use Case |
|--------|-------------|-----------|----------|
| FP32 | 31 | âœ“ | Baseline, not recommended |
| FP16/BF16 | 62 | âœ“ | Standard inference |
| FP8 E4M3/E5M2 | 124 | âœ“ | 2Ã— speedup, minimal loss |
| INT8 | 124 | âœ“ | 2Ã— speedup, PTQ required |
| INT4 | 248 | âœ“ | 4Ã— speedup, more loss |
| NVFP4 | **1000** | âœ“âœ“ | 4Ã— speedup, Blackwell optimized! |
| MXFP4 | 1000 | âœ“ | Similar to NVFP4 |

### Comparison: Other GPUs

| Format | A100 | H100 | B10 | B200 |
|--------|------|------|-----|------|
| FP32 | âœ“ | âœ“ | âœ“ | âœ“ |
| FP16/BF16 | âœ“ TC | âœ“ TC | âœ“ TC | âœ“ TC |
| FP8 | âœ— | âœ“ TC | âœ“ TC | âœ“ TC |
| NVFP4 | âœ— | âœ— | âœ“ TC | âœ“ TC |
| INT8 | âœ“ TC | âœ“ TC | âœ“ TC | âœ“ TC |

**TC** = Tensor Core support

---

## Key Takeaways

1. **Memory-bound dominance:** Transformer inference has AI < 10 for most ops, far below GB10's critical AI (216+ for FP16)

2. **Precision speedup = bytes reduction** in memory-bound regime:
   - FP16 â†’ FP8: 2Ã— faster
   - FP16 â†’ INT4/NVFP4: ~4Ã— faster

3. **GB10's NVFP4 advantage:** 1000 TFLOPS at 4-bit precision with minimal accuracy loss

4. **KV cache becomes bottleneck** at long context (>4K tokens for 8B models)

5. **Validation is essential:** Roofline predicts ideal performance; real kernels may differ by 10-20%

## References

- [Roofline Model (Williams et al.)](https://people.eecs.berkeley.edu/~kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf)
- [OCP MX Specification](https://www.opencompute.org/documents/ocp-microscaling-formats-mx-v1-0-spec-final-pdf)
- [NVIDIA FP8 Formats](https://docs.nvidia.com/deeplearning/transformer-engine/user-guide/index.html)
- [TorchAO Quantization](https://github.com/pytorch/ao)
